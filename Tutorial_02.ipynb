{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44b28d45-4572-4cee-b577-e82931743b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\najmulu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\najmulu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c8751f-b91c-400d-9c90-b6873134c190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stemming with Porter and Snowball ===\n",
      "Word:      A\n",
      "Porter:    a\n",
      "Snowball:  a\n",
      "\n",
      "Word:      beautiful\n",
      "Porter:    beauti\n",
      "Snowball:  beauti\n",
      "\n",
      "Word:      waitress\n",
      "Porter:    waitress\n",
      "Snowball:  waitress\n",
      "\n",
      "Word:      is\n",
      "Porter:    is\n",
      "Snowball:  is\n",
      "\n",
      "Word:      running\n",
      "Porter:    run\n",
      "Snowball:  run\n",
      "\n",
      "Word:      on\n",
      "Porter:    on\n",
      "Snowball:  on\n",
      "\n",
      "Word:      a\n",
      "Porter:    a\n",
      "Snowball:  a\n",
      "\n",
      "Word:      hilly\n",
      "Porter:    hilli\n",
      "Snowball:  hilli\n",
      "\n",
      "Word:      road\n",
      "Porter:    road\n",
      "Snowball:  road\n",
      "\n",
      "Word:      with\n",
      "Porter:    with\n",
      "Snowball:  with\n",
      "\n",
      "Word:      his\n",
      "Porter:    hi\n",
      "Snowball:  his\n",
      "\n",
      "Word:      expensive\n",
      "Porter:    expens\n",
      "Snowball:  expens\n",
      "\n",
      "Word:      running\n",
      "Porter:    run\n",
      "Snowball:  run\n",
      "\n",
      "Word:      shoes\n",
      "Porter:    shoe\n",
      "Snowball:  shoe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "porter_stemmer = PorterStemmer()  \n",
    "snowball_stemmer = SnowballStemmer(\"english\") \n",
    "\n",
    "\n",
    "my_text = \"A beautiful waitress is running on a hilly road with his expensive running shoes\"\n",
    "\n",
    "\n",
    "words = word_tokenize(my_text)\n",
    "\n",
    "\n",
    "print(\"=== Stemming with Porter and Snowball ===\")\n",
    "for w in words:\n",
    "    print(\"Word:     \", w)\n",
    "    print(\"Porter:   \", porter_stemmer.stem(w))       \n",
    "    print(\"Snowball: \", snowball_stemmer.stem(w))       \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3fc3d08-4465-4260-8e57-2350447d64a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== POS Tagged Words ===\n",
      "[('A', 'DT'), ('beautiful', 'JJ'), ('waitress', 'NN'), ('is', 'VBZ'), ('running', 'VBG'), ('on', 'IN'), ('a', 'DT'), ('hilly', 'RB'), ('road', 'NN'), ('with', 'IN'), ('his', 'PRP$'), ('expensive', 'JJ'), ('running', 'NN'), ('shoes', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "tagged = pos_tag(words) \n",
    "\n",
    "print(\"=== POS Tagged Words ===\")\n",
    "print(tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb92d4a6-4ddd-4051-9037-15a912dd9d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Nouns (NN) ===\n",
      "[('waitress', 'NN'), ('road', 'NN'), ('running', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "nouns = list(filter(lambda x: x[1] == 'NN', tagged))\n",
    "print(\"=== Nouns (NN) ===\")\n",
    "print(nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98c6bfb-8efc-45ca-a2b5-d2739ea741e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Nouns and Adjectives (NN, JJ) ===\n",
      "[('beautiful', 'JJ'), ('waitress', 'NN'), ('road', 'NN'), ('expensive', 'JJ'), ('running', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "nouns_adjectives = list(filter(lambda x: x[1] in ['NN', 'JJ'], tagged))\n",
    "print(\"=== Nouns and Adjectives (NN, JJ) ===\")\n",
    "print(nouns_adjectives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "819e02c9-c2f8-489e-9f84-8efdfea26c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"GAITXT.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    gaitxt_content = f.read()\n",
    "\n",
    "gai_words = word_tokenize(gaitxt_content)\n",
    "gai_tagged = pos_tag(gai_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d09fb1e-914d-4a76-b279-f19aaf58e013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved no_determiner.txt (without DT)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_no_dt = [word for word, tag in gai_tagged if tag != 'DT']\n",
    "\n",
    "with open(\"no_determiner.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(filtered_no_dt)) \n",
    "\n",
    "print(\"Saved no_determiner.txt (without DT)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "444496bd-caef-4ed1-8274-d498425bcafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file_verbs.txt (verbs only)\n"
     ]
    }
   ],
   "source": [
    "verb_tags = {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}\n",
    "verbs = [word for word, tag in gai_tagged if tag in verb_tags]\n",
    "\n",
    "with open(\"file_verbs.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(verbs))  \n",
    "\n",
    "print(\"Saved file_verbs.txt (verbs only)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb36730-2f20-4510-b3e4-c7f70007d340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
