{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e2378fa2",
      "metadata": {
        "id": "e2378fa2"
      },
      "source": [
        "# Function Calling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad9f712",
      "metadata": {
        "id": "2ad9f712"
      },
      "source": [
        "Function calling lets you connect LLMs to external tools and APIs. Instead of generating text responses, the model determines when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the LLMs to act as a bridge between natural language and real-world actions and data. The primary use cases of Function calling include but not limited to:\n",
        "\n",
        "- **Extend Capabilities:** Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.\n",
        "\n",
        "- **Take Actions:** Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices.\n",
        "\n",
        "\n",
        "For this tutorial, we are using Gemini free tier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "08c5956f",
      "metadata": {
        "id": "08c5956f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "291562d0-68af-44d1-9600-dd49c2eb9587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m237.3/237.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting arxiv\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.12/dist-packages (from arxiv) (2.32.4)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2025.10.5)\n",
            "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=52f45df4dfb15582e2da7e0a52d2e0885c4fd8734006aa4f141543db1fb7dbbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.2.0 feedparser-6.0.12 sgmllib3k-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-genai\n",
        "!pip install arxiv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "760248a0",
      "metadata": {
        "id": "760248a0"
      },
      "source": [
        "**`Restart the Session!`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c2b31d43",
      "metadata": {
        "id": "c2b31d43"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "import os\n",
        "import json\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6e3-ohC6HFBB",
      "metadata": {
        "id": "6e3-ohC6HFBB"
      },
      "outputs": [],
      "source": [
        "GOOGLE_API_KEY = \"AIzaSyDg8k0T-TYvQeVVDl2xHT5PEcd7hJSw53w\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "24a1647e",
      "metadata": {
        "id": "24a1647e"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-flash\"\n",
        "\n",
        "client = genai.Client(\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27317896",
      "metadata": {
        "id": "27317896"
      },
      "source": [
        "# Part 1 - Function Schema Declaration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53808f45",
      "metadata": {
        "id": "53808f45"
      },
      "source": [
        "Function calling involves a structured interaction between your application, the model, and external functions. Here's a breakdown of the process:\n",
        "\n",
        "1. **Define Function Declaration:** Define the function declaration in your application code. Function Declarations describe the function's name, parameters, and purpose to the model\n",
        "\n",
        "2. **Call LLM with function declarations:** Send user prompt along with the function declaration(s) to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.\n",
        "\n",
        "3. **Execute Function Code (Your Responsibility):** The Model does not execute the function itself. It's your application's responsibility to process the response and check for Function Call, if\n",
        "\n",
        "    - **Yes**: Extract the name and args of the function and execute the corresponding function in your application.\n",
        "\n",
        "    - **No**: The model has provided a direct text response to the prompt (this flow is less emphasized in the example but is a possible outcome).\n",
        "\n",
        "4. **Send the results back to LLM:** If a function was executed, capture the result and send it back to the model in a subsequent turn of the conversation. It will use the result to generate a final, user-friendly response that incorporates the information from the function call."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "925a5f64",
      "metadata": {
        "id": "925a5f64"
      },
      "source": [
        "### Step 1 - Define Function Declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b2410d01",
      "metadata": {
        "id": "b2410d01"
      },
      "outputs": [],
      "source": [
        "# Define the function declaration for the model\n",
        "weather_function = {\n",
        "    \"name\": \"get_current_temperature\",\n",
        "    \"description\": \"Gets the current temperature for a given location.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"location\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The city name, e.g. San Francisco\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"location\"],\n",
        "    },\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edcc7052",
      "metadata": {
        "id": "edcc7052"
      },
      "source": [
        "### Step 2 - Call LLM with function declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c47f25f6",
      "metadata": {
        "id": "c47f25f6"
      },
      "outputs": [],
      "source": [
        "tools = types.Tool(function_declarations=[weather_function])\n",
        "config = types.GenerateContentConfig(tools=[tools])\n",
        "\n",
        "# Send request with function declarations\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"what is the temperature in Sydney?\",\n",
        "    config=config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fa20978",
      "metadata": {
        "id": "2fa20978"
      },
      "source": [
        "### Check the model response\n",
        "\n",
        "**Note:** Check the output from Gemini, which is not a normal text response, but the structured output indicating the function should be called by name, and the required parameters to call the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9eea7da0",
      "metadata": {
        "id": "9eea7da0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c5831d-bf24-489f-b9db-ddf34ff79343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FunctionCall(\n",
            "  args={\n",
            "    'location': 'Sydney'\n",
            "  },\n",
            "  name='get_current_temperature'\n",
            ")]\n"
          ]
        }
      ],
      "source": [
        "print(response.function_calls)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a58effb4",
      "metadata": {
        "id": "a58effb4"
      },
      "source": [
        "## Function call in more details"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dc5d4a5",
      "metadata": {
        "id": "9dc5d4a5"
      },
      "source": [
        "### Function Declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "41cb53a5",
      "metadata": {
        "id": "41cb53a5"
      },
      "outputs": [],
      "source": [
        "# Define the function declaration for the model\n",
        "\n",
        "\n",
        "lighting_function = {\n",
        "    \"description\": \"Turn on the lighting system.\",\n",
        "    \"name\": \"enable_lights\"\n",
        "}\n",
        "\n",
        "set_light_color_function = {\n",
        "    \"description\": \"Set the light color. Must enable lights first.\",\n",
        "    \"name\": \"set_light_color\",\n",
        "    \"parameters\": {\n",
        "        \"properties\": {\n",
        "            \"color\": {\n",
        "                \"type\": \"STRING\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\n",
        "            \"color\"\n",
        "        ],\n",
        "        \"type\": \"OBJECT\"\n",
        "    }\n",
        "}\n",
        "\n",
        "instruction = \"\"\"\n",
        "  You are a helpful bot.\n",
        "  You have the following capabilities:\n",
        "  - Turn lights on\n",
        "  - Set the color of the lights. Must enable lights first.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c8f04093",
      "metadata": {
        "id": "c8f04093"
      },
      "outputs": [],
      "source": [
        "# The actual function\n",
        "\n",
        "def enable_lights():\n",
        "    \"\"\"Turn on the lighting system.\"\"\"\n",
        "    return \"LIGHTBOT: Lights enabled.\"\n",
        "\n",
        "def set_light_color(color: str):\n",
        "    \"\"\"Set the light color. Lights must be enabled for this to work.\"\"\"\n",
        "    return f\"LIGHTBOT: Lights set to {color}.\"\n",
        "\n",
        "\n",
        "tool_map = {\n",
        "    \"enable_lights\": enable_lights,\n",
        "    \"set_light_color\": set_light_color,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52828425",
      "metadata": {
        "id": "52828425"
      },
      "source": [
        "### Call LLM with function declarations - only the declarations are passed to LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ffdce61f",
      "metadata": {
        "id": "ffdce61f"
      },
      "outputs": [],
      "source": [
        "user_query = \"I like the yellow light.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "babab2a8",
      "metadata": {
        "id": "babab2a8"
      },
      "outputs": [],
      "source": [
        "tools = types.Tool(function_declarations=[lighting_function, set_light_color_function])\n",
        "config = types.GenerateContentConfig(tools=[tools], system_instruction=instruction)\n",
        "\n",
        "# Send request with function declarations\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=user_query,\n",
        "    config=config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d561cf64",
      "metadata": {
        "id": "d561cf64"
      },
      "source": [
        "### Check the model response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "80596369",
      "metadata": {
        "id": "80596369",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39163066-d9b6-43ae-cbde-e1f9e13af12e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FunctionCall(\n",
            "  args={},\n",
            "  name='enable_lights'\n",
            ")]\n"
          ]
        }
      ],
      "source": [
        "print(response.function_calls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4aa4deae",
      "metadata": {
        "id": "4aa4deae"
      },
      "outputs": [],
      "source": [
        "tool_part_1 = response.candidates[0].content.parts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "65602c87",
      "metadata": {
        "id": "65602c87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40465ea4-4d7b-4908-9991-2737ec098dd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video_metadata=None thought=None inline_data=None file_data=None thought_signature=b'\\n\\xe7\\x02\\x01\\xd1\\xed\\x8ao\\xd5P/\\xfe\\x83\\x97\\xf4\\x1e\\xd2l\\xff\\xe1\\x9d\\\\\\xde\\xb6%~\\xd9Q\\x16@\\x8d.\\x06\\\\3\\x0bz NB\\x1f\\xb0t@\\xcfm\\x18Bv\\x84\\xe2N\\x8fx>\\x9c\\xcbY\\xe1&K\\xb0\\xdb\\x8al\\xe7\\xd7f\\x19-\\xa3\\xf9\\xe9\\x10!\\xc3\\xaf\\x1b\\x9dIhlf\\xdd\\'-\\x82\\x8dNE9D\\x84\\x08_w#{\\x0c*\\xd6d?\\xa1\\x86\\xf4\\xae\\r\\x0c\\x00\\r\\xfb\\x87\\x88\\xbf\\xabM\\r1R6A\\x0e\\x1b\\xf3=Gtd\\xef\\x93\\x042\\xc6\\xab\\nW\\xc3\\xbdZeUn\\xca\\x13\\x89\\x8e\\x88g8\\xef\\x90\\xce\\xdb\\x81p\\x17\\x83\\x96\\xc3H\\xf8;\\'\\xb4\\\\\\xc3z\\xbc\\x8e\\xe9M1\\x93f\\xea\\x0e\\xc2\\x1a7O\\xc1@\\x85\\xcelw\\xf6\\x91\\xb5\\x89\\x81\\xb3\\x90\\xe3\\x1fM\\xeaLoW`\\x1e&~\\xc4\\xfb\\xee\\x11\\xda\\xaeW\\xbb\\xec;P\\xa6\\xc2,Rn\\xe6[\\x0f\\xfc\\x1e\\x92(2\\xce\\x18Ec`\\xbe. \\x95~\\xe1s\\xeb\\xe0j\\'/7\\x8e\\xb4\\x81\\xe6\\x166\\xac\\xa2,\\x15}\\x1a\\x07\\xd8\\xaeT\\x82\\xb5\\x88xG\\x1d\\xc3\\xad\\xf0\\xb4\"Z\\xf1\\xdb\\xf8F<[\\xfb\\xf3[\\xfb\\xe2+\\xb6pqr\\x89\\x0f\\x96\\x9f\\x81f\\x1f\\x86\\x8d\\t\\x05\\xa2\\x90\\xbaP\\x86@$Z\\xc1\\xe0\\x01\\xda)\\x93\\xd4-;\\x05D\\x99\\xe1[\\xc3K\\xac\\x8f(+\\xffg\\xdc~dm\\x9b\\x1f\\xd2\\xbdu\\xa1|\\x0bj\\xeb\\xed\\n\\x9fPF@\\xfe\\xfdu\\xf0=\\xecB\\xdbJN' function_call=FunctionCall(\n",
            "  args={},\n",
            "  name='enable_lights'\n",
            ") code_execution_result=None executable_code=None function_response=None text=None\n"
          ]
        }
      ],
      "source": [
        "print(tool_part_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e50a39cb",
      "metadata": {
        "id": "e50a39cb"
      },
      "source": [
        "### Step 3 - Execute Fuunction Code (Manually)\n",
        "\n",
        "`Note\"`: Since we already check the `tool_part_1`, and there is only one function was called, it's fine to write the following code to call the function.\n",
        "\n",
        "Otherwise need a loop here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e9777832",
      "metadata": {
        "id": "e9777832"
      },
      "outputs": [],
      "source": [
        "function_name = tool_part_1.function_call.name\n",
        "args = tool_part_1.function_call.args\n",
        "tool_result_1 = tool_map[function_name](**args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "517f76fa",
      "metadata": {
        "id": "517f76fa"
      },
      "source": [
        "### Check the results returned by called function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0c4e1f15",
      "metadata": {
        "id": "0c4e1f15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139be909-e7ec-467a-f700-ad7a661c36a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LIGHTBOT: Lights enabled.\n"
          ]
        }
      ],
      "source": [
        "print(tool_result_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b53009c",
      "metadata": {
        "id": "2b53009c"
      },
      "source": [
        "### Step 4 - Send the function call result back to LLM\n",
        "\n",
        "`Note:` it's **important** to understand how `\"message\"` is constructed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "313e52f4",
      "metadata": {
        "id": "313e52f4"
      },
      "outputs": [],
      "source": [
        "from google.genai import types\n",
        "# Build the message history\n",
        "messages = [\n",
        "    types.Content(\n",
        "        role=\"user\",\n",
        "        parts=[\n",
        "            types.Part(\n",
        "                text=user_query\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        "    types.Content(\n",
        "        role=\"model\",\n",
        "        parts=[tool_part_1]\n",
        "    ),\n",
        "    types.Content(\n",
        "        role=\"tool\",\n",
        "        parts=[\n",
        "            types.Part.from_function_response(\n",
        "                name=tool_part_1.function_call.name,\n",
        "                response={\"output\" : tool_result_1},\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "]\n",
        "\n",
        "# Generate the next response\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=messages,\n",
        "    config=config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "49df7086",
      "metadata": {
        "id": "49df7086"
      },
      "outputs": [],
      "source": [
        "tool_part_2 = response.candidates[0].content.parts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ddcd3b2b",
      "metadata": {
        "id": "ddcd3b2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b66b72d-d7c6-411c-ed32-ae96be798ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video_metadata=None thought=None inline_data=None file_data=None thought_signature=b'\\n\\xc5\\x01\\x01\\xd1\\xed\\x8ao\\xd2\\x95\\x1a\\x81\\xa3\\x82\\xd6\\xdf\\xf5a\\x06\\x92\\xb1\\xab.\\xde\\xbb\\x92\\xeaQ\\x98\\xa2!2?\\xb9\\xaa\\xc3HT\\x0c\\xc3\\tiV\\xa9\\x16N\\x055c\\x0b\\xb4\\x88\\xaa\\xa1\\x8fR\\x88\\xf7\\x96+\\xf7\\xf4\\xe9\\x1f\\xf8p\\x13\\xc5\\xe7dh\\x8dSyD\\x9d\\xba\\xa5\\xcfw\\xce\\tq\\xb0\\xf1D\\x85\\xc6\\n\\x8d#\\xe3\\xed\\xdfM\\xc9O\\xed\\xddWE+\\xc5\\xe4\\x14\\xcb\\xdd\\xa6.\\xa1\\xbaq\\xc5\\xc4\\xbeTUE\\xbd\\x9ar\\xed@K\\xe6\\x05{\\xf9\\xee&\\xba\\xc6\\x7f\\xda\\x8f\\x7f!\\xf0u\\t\\x02a\\x83]\\x81\\x04\\x06\\x1f\\xdc\\xac-\\xa1k\\xbd\\x7f8\\xbd>\\xdc\\x9aw\\x7f<\\xf2\\x1a|\\x14\\xdb\\xe7\\x1e\\x14a\\xc0\\x80\\xa2\\x88\\xd0\\xfc\\x9e\\x11>\\x984C\\xe4\\xcf\\xd4\\x99\\xc0o\\x88o\\xd5\\x92\\xe0\\x0f\\xe1@\\xd5q' function_call=FunctionCall(\n",
            "  args={\n",
            "    'color': 'yellow'\n",
            "  },\n",
            "  name='set_light_color'\n",
            ") code_execution_result=None executable_code=None function_response=None text=None\n"
          ]
        }
      ],
      "source": [
        "print(tool_part_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e519f73f",
      "metadata": {
        "id": "e519f73f"
      },
      "source": [
        "### Manually call the function and get the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "7d7233be",
      "metadata": {
        "id": "7d7233be"
      },
      "outputs": [],
      "source": [
        "function_name = tool_part_2.function_call.name\n",
        "args = tool_part_2.function_call.args\n",
        "tool_result_2 = tool_map[function_name](**args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1de2d5be",
      "metadata": {
        "id": "1de2d5be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f334118c-ea71-4471-a522-b8dea49b4a03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LIGHTBOT: Lights set to yellow.\n"
          ]
        }
      ],
      "source": [
        "print(tool_result_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14eeca30",
      "metadata": {
        "id": "14eeca30"
      },
      "source": [
        "### Send the result back to LLM.\n",
        "\n",
        " **Must** include all the history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "08a94323",
      "metadata": {
        "id": "08a94323"
      },
      "outputs": [],
      "source": [
        "from google.genai import types\n",
        "# Build the message history\n",
        "messages = [\n",
        "    types.Content(\n",
        "        role=\"user\",\n",
        "        parts=[\n",
        "            types.Part(\n",
        "                text=user_query\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        "    types.Content(\n",
        "        role=\"model\",\n",
        "        parts=[tool_part_1]\n",
        "    ),\n",
        "    types.Content(\n",
        "        role=\"tool\",\n",
        "        parts=[\n",
        "            types.Part.from_function_response(\n",
        "                name=tool_part_1.function_call.name,\n",
        "                response={\"output\":tool_result_1},\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        "    types.Content(\n",
        "        role=\"model\",\n",
        "        parts=[tool_part_2]\n",
        "    ),\n",
        "    types.Content(\n",
        "        role=\"tool\",\n",
        "        parts=[\n",
        "            types.Part.from_function_response(\n",
        "                name=tool_part_2.function_call.name,\n",
        "                response={\"output\":tool_result_2},\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "]\n",
        "\n",
        "# Generate the next response\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=messages,\n",
        "    config=config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04e90c75",
      "metadata": {
        "id": "04e90c75"
      },
      "source": [
        "### Check the model response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d476ec24",
      "metadata": {
        "id": "d476ec24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4165f30e-14c4-4fea-e77f-e40e6133024c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alright, the lights are now yellow.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "part_3 = response.candidates[0].content.parts[0]\n",
        "print(part_3.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc0c071a",
      "metadata": {
        "id": "bc0c071a"
      },
      "source": [
        "# Part 2 - Automating the schema declaration and funcion calling\n",
        "\n",
        "In Gemini, the Python SDK's `ChatSession (client.chats.create(...))` simplifies function calling execution via its `automatic_function_calling` feature (enabled by default).\n",
        "\n",
        "OpenAI Agent SDK has similar features. `@function_tool` decorator can register your customised functions as tools.\n",
        "\n",
        "Framework like LangChain has `@tool` to register a function as a tool. Also, it has a very rich Tool list. For more information see: https://python.langchain.com/docs/integrations/tools/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4519af46",
      "metadata": {
        "id": "4519af46"
      },
      "outputs": [],
      "source": [
        "def enable_lights():\n",
        "    \"\"\"Turn on the lighting system.\"\"\"\n",
        "    print(\"LIGHTBOT: Lights enabled.\")\n",
        "\n",
        "\n",
        "def set_light_color(rgb_hex: str):\n",
        "    \"\"\"Set the light color. Lights must be enabled for this to work.\"\"\"\n",
        "    print(f\"LIGHTBOT: Lights set to {rgb_hex}.\")\n",
        "\n",
        "\n",
        "def stop_lights():\n",
        "    \"\"\"Stop flashing lights.\"\"\"\n",
        "    print(\"LIGHTBOT: Lights turned off.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ece1705",
      "metadata": {
        "id": "7ece1705"
      },
      "source": [
        "### Try the lighting system\n",
        "\n",
        "Adopt the 3 functions controlling a hypothetical lighting system. Note the docstrings and type hints, which are essential for function schema declaration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8e4bf088",
      "metadata": {
        "id": "8e4bf088"
      },
      "outputs": [],
      "source": [
        "# create the light control toolkit, usting the previous 3 light control functions\n",
        "light_controls = [enable_lights, set_light_color, stop_lights]\n",
        "\n",
        "\n",
        "# define the instructions for the model to decide when to use which light control tool\n",
        "instruction = \"\"\"\n",
        "  You are a helpful lighting system bot. You can turn\n",
        "  lights on and off, and you can set the color. Do not perform any\n",
        "  other tasks.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "3f85941e",
      "metadata": {
        "id": "3f85941e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b3bc0bf-d143-4707-f76f-0dc67e25f4f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LIGHTBOT: Lights enabled.\n",
            "Ok, I've turned on the lights.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# create the chat session\n",
        "chat = client.chats.create(\n",
        "    model=MODEL_ID,\n",
        "    config={\n",
        "        \"tools\": light_controls,\n",
        "        \"system_instruction\": instruction,\n",
        "        # automatic_function_calling defaults to enabled\n",
        "    }\n",
        ")\n",
        "\n",
        "response = chat.send_message(\"It's awful dark in here...\")\n",
        "\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FSUPjT9P8OUE",
      "metadata": {
        "id": "FSUPjT9P8OUE"
      },
      "source": [
        "# **Important!**\n",
        "\n",
        "Only complete the code where it shows `# TODO`; do **NOT** change other parts or rewrite the entire function."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "219584ad",
      "metadata": {
        "id": "219584ad"
      },
      "source": [
        "## Assessment 1 - Query the lighting system"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c547bc86",
      "metadata": {
        "id": "c547bc86"
      },
      "source": [
        "Use the lighting control system, design your own queries, 2 of them, which will call `set_light_color` and `stop_lights` respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "f3a5efeb",
      "metadata": {
        "id": "f3a5efeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d4c043-5ab3-4fdc-c87a-87e341a3c2c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LIGHTBOT: Lights set to #00FF00.\n",
            "I've changed the lights to green.\n",
            "LIGHTBOT: Lights turned off.\n",
            "The lights are now off.\n"
          ]
        }
      ],
      "source": [
        "# Design your own queries for the lighting control system\n",
        "\n",
        "# TODO design your own query here, 2 of them\n",
        "query = [\n",
        "    \"Can you change the lights to green, please?\",\n",
        "    \"Turn off all the lights now, thank you.\"\n",
        "]\n",
        "\n",
        "\n",
        "chat = client.chats.create(\n",
        "    model=MODEL_ID,\n",
        "    config={\n",
        "        \"tools\": light_controls,\n",
        "        \"system_instruction\": instruction,\n",
        "    }\n",
        ")\n",
        "\n",
        "for q in query:\n",
        "    response = chat.send_message(q)\n",
        "    print(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb2cc9de",
      "metadata": {
        "id": "fb2cc9de"
      },
      "source": [
        "## Assessment 2 - Your own design\n",
        "\n",
        "Must use `client.models.generate_content` mode and manually execute the function.\n",
        "\n",
        "1. Design your own new system (hypothetical system) with real business logic\n",
        "\n",
        "2. Design 2-3 functions (dummy functions, no actual functionality required, exactly the same format as the lighting functions) which are relevant to your system functionality\n",
        "\n",
        "3. Design 2-3 queries to test that LLMs will call the functions you designed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U56uQaHt-w0o",
      "metadata": {
        "id": "U56uQaHt-w0o"
      },
      "outputs": [],
      "source": [
        "# TODO: provide the system description here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "769f2398",
      "metadata": {
        "id": "769f2398"
      },
      "outputs": [],
      "source": [
        "# TODO design your own hypothetical system with 2-3 functions\n",
        "\n",
        "\n",
        "def todo_function1(): # must modify the function name and docstring and the returned message\n",
        "    \"\"\"TODO: design your own function here\"\"\"\n",
        "\n",
        "\n",
        "def todo_function2(): # must modify the function name and docstring and the returned message\n",
        "    \"\"\"TODO: design your own function here\"\"\"\n",
        "\n",
        "\n",
        "# if only 2 functions are required, you can remove the last one\n",
        "def todo_function3(): # must modify the function name and docstring and the returned message\n",
        "    \"\"\"TODO: design your own function here\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IOI1NuS_vCZu",
      "metadata": {
        "id": "IOI1NuS_vCZu"
      },
      "outputs": [],
      "source": [
        "# TODO Declare the functions your just designed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TFd0IjEXu79R",
      "metadata": {
        "id": "TFd0IjEXu79R"
      },
      "outputs": [],
      "source": [
        "# TODO design the instructions for the model to decide when to use which tool\n",
        "instruction = \"\"\"\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LaPCH1yPxViW",
      "metadata": {
        "id": "LaPCH1yPxViW"
      },
      "outputs": [],
      "source": [
        "# Bind the tools with LLMs\n",
        "\n",
        "\n",
        "# TODO: define the too_map\n",
        "\n",
        "tool_map = {}\n",
        "\n",
        "\n",
        "# TODO: define the tools and config\n",
        "tools = types.Tool(function_declarations=[\"\"\"List your tools declarations here\"\"\"])\n",
        "config = types.GenerateContentConfig(tools=[tools], system_instruction=instruction)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2344397",
      "metadata": {
        "id": "c2344397"
      },
      "outputs": [],
      "source": [
        "# Design 2-3 queries so that LLMs will call the functions you designed\n",
        "\n",
        "# TODO design your own query here, 2-3 of them\n",
        "query = []\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C8dOeS1-x7nr",
      "metadata": {
        "id": "C8dOeS1-x7nr"
      },
      "outputs": [],
      "source": [
        "# Complete the chatbot function to address the user query and manually handle the tool calling\n",
        "\n",
        "def chatbot(user_query):\n",
        "\n",
        "  response = client.models.generate_content(\n",
        "      model=MODEL_ID,\n",
        "      contents=user_query,\n",
        "      config=config,\n",
        "  )\n",
        "\n",
        "  final_response = response.text\n",
        "\n",
        "\n",
        "# TODO handle the model response and manually execute the function calling\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return final_response\n",
        "\n",
        "\n",
        "for q in query:\n",
        "  print(chatbot(q))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Assessment 2 - Smart Fitness Assistant System\n",
        "# ============================================================\n",
        "\n",
        "# This system helps users log workouts, track daily water intake,\n",
        "# and provides motivational quotes. It demonstrates manual\n",
        "# function-calling using Gemini via client.models.generate_content().\n",
        "# ============================================================\n",
        "\n",
        "# 1Ô∏è‚É£ Define the system functions\n",
        "def log_workout(workout_type: str, duration: int):\n",
        "    \"\"\"Logs the user's workout type and duration in minutes.\"\"\"\n",
        "    return f\"FITBOT: Logged your {duration}-minute {workout_type} session. Great job! üèãÔ∏è‚Äç‚ôÇÔ∏è\"\n",
        "\n",
        "\n",
        "def track_water_intake(litres: float):\n",
        "    \"\"\"Tracks the user's daily water intake.\"\"\"\n",
        "    if litres < 1:\n",
        "        return \"FITBOT: Remember to drink more water! Hydration keeps you energized. üíß\"\n",
        "    elif litres < 2:\n",
        "        return f\"FITBOT: You've had {litres} L ‚Äî almost there! Keep sipping!\"\n",
        "    else:\n",
        "        return f\"FITBOT: Excellent! {litres} L of water logged today. ‚úÖ\"\n",
        "\n",
        "\n",
        "def show_motivation():\n",
        "    \"\"\"Displays a motivational fitness quote.\"\"\"\n",
        "    return \"FITBOT: 'Push yourself because no one else is going to do it for you.' üí™\"\n",
        "\n",
        "\n",
        "# 2Ô∏è‚É£ Declare the functions as Gemini tools\n",
        "tools = types.Tool(function_declarations=[\n",
        "    {\n",
        "        \"name\": \"log_workout\",\n",
        "        \"description\": \"Logs a workout session with its type and duration.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"workout_type\": {\"type\": \"string\"},\n",
        "                \"duration\": {\"type\": \"integer\"}\n",
        "            },\n",
        "            \"required\": [\"workout_type\", \"duration\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"track_water_intake\",\n",
        "        \"description\": \"Tracks how many litres of water the user drank.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"litres\": {\"type\": \"number\"}\n",
        "            },\n",
        "            \"required\": [\"litres\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"show_motivation\",\n",
        "        \"description\": \"Provides a motivational fitness quote to inspire the user.\"\n",
        "    }\n",
        "])\n",
        "\n",
        "# 3Ô∏è‚É£ Instruction for Gemini\n",
        "instruction = \"\"\"\n",
        "You are FITBOT ‚Äî a friendly Smart Fitness Assistant.\n",
        "You can:\n",
        "- Log a workout by type and duration.\n",
        "- Track a user's daily water intake (in litres).\n",
        "- Share a motivational quote when requested.\n",
        "Only use these tools for fitness-related queries.\n",
        "\"\"\"\n",
        "\n",
        "# 4Ô∏è‚É£ Map tool names to real Python functions\n",
        "tool_map = {\n",
        "    \"log_workout\": log_workout,\n",
        "    \"track_water_intake\": track_water_intake,\n",
        "    \"show_motivation\": show_motivation\n",
        "}\n",
        "\n",
        "# 5Ô∏è‚É£ Configure the model with tools and instructions\n",
        "config = types.GenerateContentConfig(tools=[tools], system_instruction=instruction)\n",
        "\n",
        "# 6Ô∏è‚É£ Create example user queries\n",
        "query = [\n",
        "    \"I just finished a 45 minute yoga session.\",\n",
        "    \"I drank 1.5 litres of water today.\",\n",
        "    \"Give me some workout motivation.\"\n",
        "]\n",
        "\n",
        "# 7Ô∏è‚É£ Chatbot function - manual function calling\n",
        "def chatbot(user_query):\n",
        "    # Send the query to Gemini\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=user_query,\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    # Default response (if no function is called)\n",
        "    final_response = response.text\n",
        "\n",
        "    # Handle and execute Gemini's function calls manually\n",
        "    if response.function_calls:\n",
        "        for fc in response.function_calls:\n",
        "            fname = fc.name\n",
        "            args = fc.args\n",
        "            print(f\"Function to call: {fname}, Args: {args}\")\n",
        "\n",
        "            # Execute the appropriate local Python function\n",
        "            result = tool_map[fname](**args) if args else tool_map[fname]()\n",
        "            final_response = result\n",
        "\n",
        "    return final_response\n",
        "\n",
        "\n",
        "# 8Ô∏è‚É£ Run and test queries\n",
        "for q in query:\n",
        "    print(chatbot(q))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owGmHpOq0njV",
        "outputId": "57237ba8-a66c-4d49-9241-658dab8effd3"
      },
      "id": "owGmHpOq0njV",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature', 'function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function to call: log_workout, Args: {'workout_type': 'yoga', 'duration': 45}\n",
            "FITBOT: Logged your 45-minute yoga session. Great job! üèãÔ∏è‚Äç‚ôÇÔ∏è\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature', 'function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function to call: track_water_intake, Args: {'litres': 1.5}\n",
            "FITBOT: You've had 1.5 L ‚Äî almost there! Keep sipping!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature', 'function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function to call: show_motivation, Args: {}\n",
            "FITBOT: 'Push yourself because no one else is going to do it for you.' üí™\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb1ab640",
      "metadata": {
        "id": "bb1ab640"
      },
      "source": [
        "# Part 3 - APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4575577",
      "metadata": {
        "id": "f4575577"
      },
      "source": [
        "Google Search can be connected to Gemini to search real-time web content. This allows Gemini answer questions about recent topics.\n",
        "\n",
        "In our context:\n",
        "\n",
        "- **Customized Functions:** These are user-defined Python functions where the core logic or functionality is implemented by the user. For example, a function that performs a calculation, processes data locally, or implements a specific algorithm entirely within the user‚Äôs code\n",
        "\n",
        "- **API-Based Tools:** These are user-defined Python functions that serve as wrappers to interact with third-party APIs (can be Python package or HTTP API call), where the core functionality (e.g., data retrieval, processing) is provided by the external API, developed and maintained by others.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "ce9a0c9d",
      "metadata": {
        "id": "ce9a0c9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862d0365-b51c-44e0-e26a-6012c7a5bd74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Sydney, Australia, as of Wednesday, October 15, 2025, at 11:25 PM, the weather is clear. The temperature is 69¬∞F (21¬∞C), but it feels like 76¬∞F (24¬∞C), with humidity around 80%. There is currently a 0% chance of rain.\n",
            "\n",
            "The forecast for the rest of Wednesday night is clear.\n",
            "\n",
            "Here's a brief outlook for the next few days:\n",
            "*   **Thursday, October 16:** Sunny with temperatures between 71¬∞F (22¬∞C) and 82¬∞F (28¬∞C), and a 5% chance of rain.\n",
            "*   **Friday, October 17:** Scattered thunderstorms are expected during the day with light rain at night, and a 40-45% chance of rain. Temperatures will range from 63¬∞F (17¬∞C) to 87¬∞F (31¬∞C).\n",
            "*   **Saturday, October 18:** Light rain is possible during the day, becoming mostly cloudy at night, with a 20% chance of rain. Temperatures will be between 62¬∞F (17¬∞C) and 71¬∞F (22¬∞C).\n"
          ]
        }
      ],
      "source": [
        "# Define the grounding tool\n",
        "\n",
        "google_search = types.GoogleSearch()\n",
        "\n",
        "grounding_tool = types.Tool(\n",
        "    google_search=google_search\n",
        ")\n",
        "\n",
        "# Configure generation settings\n",
        "config = types.GenerateContentConfig(\n",
        "    tools=[grounding_tool]\n",
        ")\n",
        "\n",
        "# Make the request\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"what is the weather like in Sydney?\",\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "# Print the grounded response\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e050eb2",
      "metadata": {
        "id": "9e050eb2"
      },
      "source": [
        "# Assessment 3 - Implement arxiv API tool"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "358b1946",
      "metadata": {
        "id": "358b1946"
      },
      "source": [
        "Equiping your Gemini model so that it can search research paper on ArXiv.\n",
        "\n",
        "Check the following documentation for arxiv API and Python package build on it.\n",
        "\n",
        "HTTP API:\n",
        "https://info.arxiv.org/help/api/basics.html#python_simple_example\n",
        "\n",
        "Python Package:\n",
        "https://pypi.org/project/arxiv/\n",
        "\n",
        "\n",
        "Langchain also has tool wrapper for ArXiv tool, for more information see:\n",
        "https://python.langchain.com/docs/integrations/tools/arxiv/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b9ff563",
      "metadata": {
        "id": "1b9ff563"
      },
      "outputs": [],
      "source": [
        "!pip install arxiv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NYzwpaOZsNI1",
      "metadata": {
        "id": "NYzwpaOZsNI1"
      },
      "outputs": [],
      "source": [
        "import arxiv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "522bc610",
      "metadata": {
        "id": "522bc610"
      },
      "outputs": [],
      "source": [
        "# Define arxiv search function here to wrap the API or Python package\n",
        "\n",
        "def search_arxiv(query: str, max_results: int = 2) -> list: # max_results set to 2 for testing\n",
        "    \"\"\"\n",
        "    Search arXiv for research papers based on a query string.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query for arXiv papers.\n",
        "        max_results (int): Maximum number of results to return (default: 5).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing paper details (title, authors, abstract, etc.).\n",
        "    \"\"\"\n",
        "    # TODO implement the function here, can be Python package or HTTP API call\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b4b4e9",
      "metadata": {
        "id": "14b4b4e9"
      },
      "outputs": [],
      "source": [
        "# TODO design the instructions for the model to decide when to use this tool\n",
        "\n",
        "instruction = \"\"\"\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1df74486",
      "metadata": {
        "id": "1df74486"
      },
      "outputs": [],
      "source": [
        "# Design 2 queries to test the tool, with one query that will call the tool and one that will not\n",
        "\n",
        "tools = [search_arxiv]\n",
        "\n",
        "# TODO design your own query here, 2 of them\n",
        "query = []\n",
        "\n",
        "chat = client.chats.create(\n",
        "    model=MODEL_ID,\n",
        "    config={\n",
        "        \"tools\": tools,\n",
        "        \"system_instruction\": instruction,\n",
        "    }\n",
        ")\n",
        "\n",
        "for q in query:\n",
        "    response = chat.send_message(q)\n",
        "    print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "18ebb06b",
      "metadata": {
        "id": "18ebb06b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0a6992-0a6d-480e-db7f-2c50d6f227ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.12/dist-packages (2.2.0)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.12/dist-packages (from arxiv) (6.0.12)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.12/dist-packages (from arxiv) (2.32.4)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.12/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2025.10.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User Query: Find recent research papers on deep learning optimization.\n",
            "Response:\n",
            "Here are two recent research papers on deep learning optimization:\n",
            "\n",
            "1.  **\"Deep Inverse Optimization\"** by Yingcong Tan, Andrew Delong, and Daria Terekhov. This paper casts inverse optimization as a form of deep learning, where their method, called deep inverse optimization, unrolls an iterative optimization process. You can find it at: http://arxiv.org/abs/1812.00804v1\n",
            "\n",
            "2.  **\"Optimization Methods in Deep Learning: A Comprehensive Overview\"** by David Shulman. This paper provides an overview of the optimization methods used to train deep neural networks, which are crucial for the effectiveness of deep learning. You can find it at: http://arxiv.org/abs/2302.09566v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User Query: Tell me a motivational quote about teamwork.\n",
            "Response:\n",
            "\"Alone we can do so little; together we can do so much.\" - Helen Keller\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!pip install arxiv\n",
        "import arxiv\n",
        "\n",
        "# Define arxiv search function here to wrap the API or Python package\n",
        "def search_arxiv(query: str, max_results: int = 2) -> list:\n",
        "    \"\"\"\n",
        "    Search arXiv for research papers based on a query string.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query for arXiv papers.\n",
        "        max_results (int): Maximum number of results to return (default: 2).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing paper details.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    # Query ArXiv API\n",
        "    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n",
        "\n",
        "    for result in search.results():\n",
        "        results.append({\n",
        "            \"title\": result.title,\n",
        "            \"authors\": [a.name for a in result.authors],\n",
        "            \"summary\": result.summary[:300] + \"...\",\n",
        "            \"url\": result.entry_id\n",
        "        })\n",
        "\n",
        "    # Return formatted result list\n",
        "    return results\n",
        "\n",
        "\n",
        "# Instruction for Gemini to decide when to use the tool\n",
        "instruction = \"\"\"\n",
        "You are a research assistant bot that helps users find academic papers.\n",
        "Use the ArXiv tool to search for research publications whenever the query\n",
        "mentions academic topics, research, machine learning, AI, data science, or algorithms.\n",
        "Otherwise, just answer normally.\n",
        "\"\"\"\n",
        "\n",
        "# Bind the tool\n",
        "tools = [search_arxiv]\n",
        "\n",
        "# Two queries: one will trigger the tool, one will not\n",
        "query = [\n",
        "    \"Find recent research papers on deep learning optimization.\",\n",
        "    \"Tell me a motivational quote about teamwork.\"\n",
        "]\n",
        "\n",
        "# Create chat session with Gemini + ArXiv tool\n",
        "chat = client.chats.create(\n",
        "    model=MODEL_ID,\n",
        "    config={\n",
        "        \"tools\": tools,\n",
        "        \"system_instruction\": instruction,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Run and test both queries\n",
        "for q in query:\n",
        "    response = chat.send_message(q)\n",
        "    print(f\"\\nUser Query: {q}\")\n",
        "    print(\"Response:\")\n",
        "    print(response.text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}